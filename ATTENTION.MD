amen@amens-Laptop RL_MultiHeadAttention % cd /Users/amen/Documents/RL_MultiHeadAttention && python test_variable_mlp.py   cd /Users/amen/Documents/RL_MultiHeadAttention && python test_variable_mlp.py
zsh: command not found: python
amen@amens-Laptop RL_MultiHeadAttention % python3 train_simple_mlp.py --algorithm TD3 --mlp-size heavy --timesteps 1000     python3 train_simple_mlp.py --algorithm TD3 --mlp-size heavy --timesteps 1000
Traceback (most recent call last):
  File "/Users/amen/Documents/RL_MultiHeadAttention/train_simple_mlp.py", line 9, in <module>
    import pandas as pd
ModuleNotFoundError: No module named 'pandas'
amen@amens-Laptop RL_MultiHeadAttention % source venv/bin/activate && python train_simple_mlp.py --algorithm TD3 --mlp-size heavy --timesteps 1000     source venv/bin/activate && python train_simple_mlp.py --algorithm TD3 --mlp-size heavy --timesteps 1000
âœ… RESTORED: Increased features per coin from 14 to 28
âœ… Total observation dimension: 84 (was 42)
âš ï¸ WARNING: Using placeholder min/max values for observation space. These must be updated.
ğŸ¯ USING TF-AGENTS EXACT TRAINING METHODOLOGY
ğŸ”§ This replicates the successful learning paradigm:
   â€¢ Collect 100 steps â†’ Train 1 iteration â†’ Repeat
   â€¢ Evaluate every 4 iterations (400 steps)
   â€¢ Exact hyperparameters from working implementation
ğŸ¯ CONSTRAINED REINFORCEMENT LEARNING ENABLED
   â€¢ Profile: balanced
   â€¢ Adaptive Lagrange multiplier Î» for smoothness constraints
   â€¢ Automatic penalty adjustment based on constraint violations
ğŸ¯ CRL Profile 'balanced': Balance between stability and responsiveness
   Constraint threshold: 0.050
   Initial Î»: 0.100
   Î» learning rate: 1.0e-03
   â€¢ Balance between stability and responsiveness
ğŸ’° REWARD SYSTEM: TRANSACTION_COST
   â€¢ Net Return = Profit - Transaction Costs
   â€¢ Transaction Cost Rate: 0.200 (20.0%)
   â€¢ Agent will learn to minimize unnecessary trading
ğŸ’¾ Models and checkpoints will be saved in: ./models/TD3_tf_agents_style_heavy_TRANSACTION_COST_variable_CRL_balanced_20250809_032856
ğŸ¯ TF-AGENTS EXACT REPLICATION WITH CONSTRAINED RL
ğŸ“Š Reward Type: TRANSACTION_COST (Net Return = Profit - Transaction Costs)
ğŸ“Š Transaction Cost Rate: 0.200 (20.0%)
ğŸ¯ CRL Profile 'balanced': Balance between stability and responsiveness
   Constraint threshold: 0.050
   Initial Î»: 0.100
   Î» learning rate: 1.0e-03
ğŸ¯ CRL Configuration:
   Profile: balanced
   Use CRL: True
   Constraint threshold: 0.0500
   Initial Î»: 0.1000
   Î» learning rate: 1.0e-03
ğŸ“Š Iterations: 1000
ğŸ“Š Steps per iteration: 100
ğŸ“Š Total steps: 100000
ğŸ“Š Evaluation every 4 iterations (400 steps)
ğŸ“Š Episodes per evaluation: 4
ğŸ¯ Creating environments with reward_type: TRANSACTION_COST
ğŸ”„ Portfolio mode: Variable
ğŸ¯ Portfolio Environment initialized with reward_type: TRANSACTION_COST
ğŸ”§ Variable portfolio setup:
   Action space: Box(0.0, 1.0, (4,), float64)
   Observation space: Dict with 84 features + 3 mask
ğŸ² Episode coins: ['LTC', 'STR', 'DASH'] (3 coins)
ğŸ’° Randomized initial allocation (variable): ['8.22%', '52.73%', '23.06%', '15.99%']
ğŸ¯ Portfolio Environment initialized with reward_type: TRANSACTION_COST
ğŸ”§ Variable portfolio setup:
   Action space: Box(0.0, 1.0, (4,), float64)
   Observation space: Dict with 84 features + 3 mask
ğŸ² Episode coins: ['STR', 'DASH', 'LTC'] (3 coins)
ğŸ’° Randomized initial allocation (variable): ['34.39%', '6.14%', '35.82%', '23.66%']
ğŸ”§ Step 1.1: Setting up SB3 logger for CSV generation...
Logging to ./logs/TD3_tf_agents_style_heavy_TRANSACTION_COST_variable_CRL_balanced_20250809_032856
âœ… SB3 logger configured with CSV output to: ./logs/TD3_tf_agents_style_heavy_TRANSACTION_COST_variable_CRL_balanced_20250809_032856
âš¡ Using OrnsteinUhlenbeckActionNoise (sigma=0.2, theta=0.15) for better exploration.
ğŸ”§ Using separate, decaying learning rates for actor and critic:
   - Actor LR: Starts at 1e-4, decays to 5e-5 over 90% of training.
   - Critic LR: Starts at 5e-4, decays to 1e-4 over 90% of training.
ğŸ¯ CRL Profile 'balanced': Balance between stability and responsiveness
   Constraint threshold: 0.050
   Initial Î»: 0.100
   Î» learning rate: 1.0e-03
Using cpu device
Wrapping the env in a DummyVecEnv.
ğŸ¯ CRL Mode Enabled:
   Constraint threshold (d): 0.0500
   Initial Î»: 0.1000
   Î» learning rate: 0.0010
ğŸ”§ Overriding optimizers with custom learning rates: Actor=0.0001, Critic=0.001
âœ… CRL Mode: Using adaptive Lagrange multiplier for smoothness constraints
âœ… Constraint: E[action_changeÂ²] â‰¤ 0.0500
âœ… STABILITY FIX: Using Huber loss for the critic.
âœ… STABILITY FIX: Using constant learning rate for direct TF-Agents replication
ğŸ”§ Step 1.2: Attaching SB3 logger to model...
âœ… Logger attached to model - CSV logging now enabled
ğŸ”§ Step 2: Setting up SB3 callbacks for TF-Agents style training...
âœ… Step 2.2: PlotGeneratorCallback configured (generate plots every 400 steps)
ğŸ§® Initial evaluation...
ğŸ² Episode coins: ['LTC', 'STR', 'DASH'] (3 coins)
ğŸ’° Randomized initial allocation (variable): ['18.82%', '0.32%', '53.55%', '27.31%']
