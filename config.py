# Configuration file for portfolio allocation RL environment
# Compatible with Stable Baselines 3 and Gymnasium

# =============================================================================
# DATA CONFIGURATION
# =============================================================================

# Data file path
FILE = "cleaned_preprocessed.csv"

# Original cryptocurrency coins (for backward compatibility)
COINS = ["DASH", "LTC", "STR"]

# =============================================================================
# VARIABLE PORTFOLIO CONFIGURATION
# =============================================================================

# Portfolio allocation mode (moved here to be used early)
USE_VARIABLE_PORTFOLIO = False  # Default to False, controlled by command line argument

# Maximum number of coins supported in a single episode
MAX_COINS = 3  # Reduced from 10 for realistic testing

# Extended coin pool to choose from (only coins available in dataset)
# Note: For demonstration of variable portfolio, we'll use subsets of available coins
COIN_POOL = ["DASH", "LTC", "STR"]

# Range of coins to select per episode (min, max) - will be set dynamically
COINS_PER_EPISODE_RANGE = (3, 3)  # Default to fixed portfolio, updated by set_portfolio_mode()

# Raw columns in the dataset
COLS = ['high', 'low', 'open', 'close', 'volume', 'quoteVolume', 'weightedAverage']

# ‚úÖ CONVERGENCE FIX: Use ONLY 30-day rolling averages like successful implementation
# This reduces complexity from 84 to 42 features, matching the working portfolio-optimization-main
SCOLS_SIMPLIFIED = ['vh', 'vl', 'vc', 'open_s', 'volume_s', 'quoteVolume_s', 'weightedAverage_s', 
                    'vh_roll_30', 'vl_roll_30', 'vc_roll_30', 'open_s_roll_30', 'volume_s_roll_30', 
                    'quoteVolume_s_roll_30', 'weightedAverage_s_roll_30']

# ‚úÖ RESTORED: Full feature set with 7, 14, and 30-day rolling averages (28 features per coin)
# As requested, this restores the original, more complex feature set.
BASE_SCOLS = ['vh', 'vl', 'vc', 'open_s', 'volume_s', 'quoteVolume_s', 'weightedAverage_s']
ROLLING_WINDOWS = ['7', '14', '30']
SCOLS = list(BASE_SCOLS)
for col in BASE_SCOLS:
    for window in ROLLING_WINDOWS:
        SCOLS.append(f'{col}_roll_{window}')

# Features per coin (calculated after SCOLS is defined)
FEATURES_PER_COIN = len(SCOLS)  # Now 28 features per coin (was 14)

# Original observation columns (for backward compatibility)
OBS_COLS = ['DASH_vh', 'LTC_vh', 'STR_vh', 'DASH_vl', 'LTC_vl', 'STR_vl', 'DASH_vc', 'LTC_vc', 'STR_vc',
            'DASH_open_s', 'LTC_open_s', 'STR_open_s', 'DASH_volume_s', 'LTC_volume_s', 'STR_volume_s', 
            'DASH_quoteVolume_s', 'LTC_quoteVolume_s', 'STR_quoteVolume_s', 'DASH_weightedAverage_s', 
            'LTC_weightedAverage_s', 'STR_weightedAverage_s', 'DASH_vh_roll_30', 'LTC_vh_roll_30', 'STR_vh_roll_30',
            'DASH_vl_roll_30', 'LTC_vl_roll_30', 'STR_vl_roll_30', 'DASH_vc_roll_30', 'LTC_vc_roll_30', 
            'STR_vc_roll_30', 'DASH_open_s_roll_30', 'LTC_open_s_roll_30', 'STR_open_s_roll_30', 
            'DASH_volume_s_roll_30', 'LTC_volume_s_roll_30', 'STR_volume_s_roll_30', 'DASH_quoteVolume_s_roll_30',
            'LTC_quoteVolume_s_roll_30', 'STR_quoteVolume_s_roll_30', 'DASH_weightedAverage_s_roll_30', 
            'LTC_weightedAverage_s_roll_30', 'STR_weightedAverage_s_roll_30']

# Variable portfolio observation space configuration
MAX_OBSERVATION_DIM = MAX_COINS * FEATURES_PER_COIN  # Maximum possible observation dimension

# ‚ö†Ô∏è PLACEHOLDER: Min/max values for the restored 28-feature set
# The original min/max values for this expanded feature set are not available.
# These are placeholders and MUST be replaced with the correct, calculated values.
SINGLE_COIN_FEATURE_MIN = [-100.0] * FEATURES_PER_COIN
SINGLE_COIN_FEATURE_MAX = [100.0] * FEATURES_PER_COIN

# Ensure we have the right number of min/max values (should be 28 now)
assert len(SINGLE_COIN_FEATURE_MIN) == FEATURES_PER_COIN, f"Min features length mismatch: {len(SINGLE_COIN_FEATURE_MIN)} != {FEATURES_PER_COIN}"
assert len(SINGLE_COIN_FEATURE_MAX) == FEATURES_PER_COIN, f"Max features length mismatch: {len(SINGLE_COIN_FEATURE_MAX)} != {FEATURES_PER_COIN}"

print(f"‚úÖ RESTORED: Increased features per coin from 14 to {FEATURES_PER_COIN}")
print(f"‚úÖ Total observation dimension: {MAX_OBSERVATION_DIM} (was {MAX_COINS * 14})")
print(f"‚ö†Ô∏è WARNING: Using placeholder min/max values for observation space. These must be updated.")

# Generate min/max arrays for maximum possible coins
OBS_COLS_MIN_VARIABLE = SINGLE_COIN_FEATURE_MIN * MAX_COINS
OBS_COLS_MAX_VARIABLE = SINGLE_COIN_FEATURE_MAX * MAX_COINS

# Legacy min/max values for backward compatibility
# NOTE: These are for the 42-feature set (14 features x 3 coins) and are no longer used
# by the variable portfolio environment with the restored feature set.
OBS_COLS_MIN = [-0.39,-0.39,-0.39,-52.92,-144.06,-159.81,-31.78,-19.57,-61.76,-87.7,-65.71,-130.35,-51.36,-51.09,-77.06,-0.03,-0.16,-74.55,-73.04,-73.72,-145.94,-0.77,-0.77,-0.77,-12.17,-14.24,-25.47,-15.64,-12.37,-12.73,-44.75,-25.01,-32.13,-39.2,-83.44,-74.04,-0.03,-0.2,-75.79,-44,-25,-32.25]
OBS_COLS_MAX = [53.16,69.71,90.97,0.4,0.4,0.4,39.31,24.4,85.49,114.25,66.32,130.35,55.72,60.25,66.09,0.04,0.18,96.32,67.32,73.63,145.94,9.96,17.79,53.89,0.81,0.81,0.81,7.58,9.9,14.93,41.44,16.35,32.11,34.64,82.48,59.62,0.03,0.2,78.16,38.94,16.29,32.24]

# =============================================================================
# ENVIRONMENT CONFIGURATION
# =============================================================================

# Episode length (number of time steps per episode)
EPISODE_LENGTH = 500

# Logging and model save directories
LOGDIR = "logs"
MODEL_SAVE = "models"

# =============================================================================
# STABLE BASELINES 3 HYPERPARAMETERS
# =============================================================================

# Training configuration
NUM_ITERATIONS = 1000
TOTAL_TIMESTEPS = 100000
EVAL_FREQ = 4  # More frequent evaluation like the successful implementation
LOG_INTERVAL = 10
MODEL_SAVE_FREQ = 12

# Evaluation settings
NUM_EVAL_EPISODES = 4  # Reduced like the successful implementation

# =============================================================================
# PPO HYPERPARAMETERS
# =============================================================================
PPO_CONFIG = {
    "learning_rate": 3e-4,
    "n_steps": 1024,        # Reduced for more frequent updates
    "batch_size": 100,      # Aligned with successful implementation
    "n_epochs": 10,
    "gamma": 0.05,         # Lower gamma like successful implementation
    "gae_lambda": 0.95,
    "clip_range": 0.2,
    "ent_coef": 0.01,
    "vf_coef": 0.5,
    "max_grad_norm": 0.5,
}

# =============================================================================
# SAC HYPERPARAMETERS  
# =============================================================================
SAC_CONFIG = {
    "learning_rate": 3e-4,
    "buffer_size": 10000,   # Smaller buffer like successful implementation
    "learning_starts": 1000,
    "batch_size": 100,      # Aligned with successful implementation
    "tau": 0.05,           # From successful implementation
    "gamma": 0.05,         # Lower gamma like successful implementation
    "train_freq": 1,
    "gradient_steps": 1,
    "ent_coef": "auto",
}

# =============================================================================
# TD3 HYPERPARAMETERS
# =============================================================================
TD3_CONFIG = {
    "learning_rate": 3e-4,
    "buffer_size": 10000,   # Smaller buffer like successful implementation
    "learning_starts": 1000,
    "batch_size": 100,      # Aligned with successful implementation
    "tau": 0.05,           # From successful implementation
    "gamma": 0.05,         # Lower gamma like successful implementation
    "train_freq": 1,
    "gradient_steps": 1,
    "policy_delay": 2,
    "target_policy_noise": 0.2,
    "target_noise_clip": 0.5,
}

# =============================================================================
# CONSTRAINED REINFORCEMENT LEARNING (CRL) HYPERPARAMETERS
# =============================================================================

# CRL configuration for TD3 with adaptive Lagrange multipliers
CRL_CONFIG = {
    # Enable/disable CRL mode
    "use_crl": True,
    
    # Constraint threshold (d): Maximum allowed average squared action change per step
    # Lower values = smoother policy, higher values = allow more aggressive changes
    "constraint_threshold": 0.05,  # Typical range: 0.01-0.1
    
    # Lagrange multiplier Œª learning rate
    # Controls how quickly Œª adapts to constraint violations
    "lambda_lr": 1e-3,  # Typical range: 1e-4 to 1e-2
    
    # Initial value for Lagrange multiplier Œª
    # Starting point for the adaptive penalty coefficient
    "initial_lambda": 0.1,  # Typical range: 0.01-1.0
    
    # Fallback fixed penalty coefficient (when CRL is disabled)
    "action_reg_coef": 0.1,
}

# Combined TD3 + CRL configuration
TD3_CRL_CONFIG = {**TD3_CONFIG, **CRL_CONFIG}

# =============================================================================
# CRL CONSTRAINT PROFILES
# =============================================================================

# Predefined constraint threshold profiles for different trading strategies
CRL_PROFILES = {
    # Very smooth trading - minimal position changes
    "conservative": {
        "constraint_threshold": 0.01,
        "initial_lambda": 0.2,
        "lambda_lr": 2e-3,
        "description": "Minimize transaction costs, very stable allocations"
    },
    
    # Balanced trading - moderate position changes allowed
    "balanced": {
        "constraint_threshold": 0.05,
        "initial_lambda": 0.1,
        "lambda_lr": 1e-3,
        "description": "Balance between stability and responsiveness"
    },
    
    # Aggressive trading - larger position changes allowed
    "aggressive": {
        "constraint_threshold": 0.15,
        "initial_lambda": 0.05,
        "lambda_lr": 5e-4,
        "description": "Allow quick responses to market changes"
    },
    
    # Adaptive trading - starts conservative, becomes more flexible
    "adaptive": {
        "constraint_threshold": 0.03,
        "initial_lambda": 0.3,
        "lambda_lr": 2e-3,
        "description": "Start stable, adapt based on market conditions"
    }
}

# =============================================================================
# DDPG HYPERPARAMETERS
# =============================================================================
DDPG_CONFIG = {
    "learning_rate": 1e-4,  # Actor learning rate from successful implementation
    "buffer_size": 10000,   # Smaller buffer like successful implementation
    "learning_starts": 1000,
    "batch_size": 100,      # Batch size from successful implementation
    "tau": 0.05,           # Target update tau from successful implementation
    "gamma": 0.05,         # Lower gamma like successful implementation
    "train_freq": 1,
    "gradient_steps": 1,
    "action_noise": None,
}

# =============================================================================
# ATTENTION MECHANISM CONFIGURATION
# =============================================================================

# Number of coins (for CoinAttentionExtractor) - now variable
N_COINS = len(COINS)  # Legacy compatibility
N_COINS_VARIABLE = MAX_COINS  # For variable portfolio

# Features per coin (for reshaping observations) - now standardized
FEATURES_PER_COIN_LEGACY = len(OBS_COLS) // N_COINS

# Attention configurations - updated for variable portfolio
ATTENTION_CONFIGS = {
    "light": {
        "features_dim": 128,
        "n_heads": 4,
        "n_layers": 1,
        "dropout": 0.1,
        "max_coins": MAX_COINS,
        "features_per_coin": FEATURES_PER_COIN  # Now correctly uses 14 features
    },
    "medium": {
        "features_dim": 256,
        "n_heads": 8,
        "n_layers": 2,
        "dropout": 0.1,
        "max_coins": MAX_COINS,
        "features_per_coin": FEATURES_PER_COIN  # Now correctly uses 14 features
    },
    "heavy": {
        "features_dim": 512,
        "n_heads": 16,  # Changed from 12 to 16 (512 √∑ 16 = 32)
        "n_layers": 3,
        "dropout": 0.2,
        "max_coins": MAX_COINS,
        "features_per_coin": FEATURES_PER_COIN  # Now correctly uses 14 features
    }
}

# =============================================================================
# PORTFOLIO ALLOCATION SPECIFIC SETTINGS
# =============================================================================

# Initial portfolio value
INITIAL_CASH = 1000

# Time delta between steps (5 minutes)
TIME_DELTA_MINUTES = 5

# Reward scaling factor
REWARD_SCALE_FACTOR = 1.0

# Transaction cost (as percentage)
TRANSACTION_COST = 0.2

# Volatility penalty weight
VOLATILITY_PENALTY_WEIGHT = 1.0  # Add this: Controls the penalty for large allocation changes.
# A higher value encourages smoother, more stable allocation strategies.

# Long-term performance bonus parameters
LONG_TERM_BONUS_ENABLED = True  # Enable/disable long-term performance bonus
LONG_TERM_LAMBDA = 1.5  # Œª_long: Weight for long-term performance bonus
LONG_TERM_LOOKBACK = 30  # N: Number of steps to look back for long-term comparison
# Formula: r_t' = r_t + Œª_long * (1/N) * (V_t - V_{t-N}) / V_{t-N}

# --- NEW: Structured Reward Parameters ---
STRUCTURED_REWARD_SCALING_FACTOR = 5.0 # Beta for tanh squashing of returns

# Type of reward function to use: "simple", "TRANSACTION_COST", "STRUCTURED_CREDIT"
REWARD_TYPE = "TRANSACTION_COST"

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def get_algorithm_config(algorithm_name):
    """Get hyperparameter configuration for specified algorithm."""
    configs = {
        "PPO": PPO_CONFIG,
        "SAC": SAC_CONFIG,
        "TD3": TD3_CONFIG,
        "DDPG": DDPG_CONFIG,
        "TD3_CRL": TD3_CRL_CONFIG,  # New CRL-enabled TD3
    }
    return configs.get(algorithm_name.upper(), PPO_CONFIG)

def get_attention_config(config_name="medium"):
    """Get attention configuration by name."""
    return ATTENTION_CONFIGS.get(config_name, ATTENTION_CONFIGS["medium"])

def get_crl_config(profile_name="balanced"):
    """
    Get CRL configuration by profile name.
    
    Args:
        profile_name: One of "conservative", "balanced", "aggressive", "adaptive"
        
    Returns:
        Dictionary with CRL hyperparameters
    """
    if profile_name not in CRL_PROFILES:
        print(f"‚ö†Ô∏è Unknown CRL profile '{profile_name}'. Available: {list(CRL_PROFILES.keys())}")
        print(f"üîÑ Using 'balanced' profile as fallback")
        profile_name = "balanced"
    
    profile = CRL_PROFILES[profile_name].copy()
    
    # Add base CRL settings
    base_crl = CRL_CONFIG.copy()
    base_crl.update(profile)
    
    print(f"üéØ CRL Profile '{profile_name}': {profile.get('description', 'No description')}")
    print(f"   Constraint threshold: {base_crl['constraint_threshold']:.3f}")
    print(f"   Initial Œª: {base_crl['initial_lambda']:.3f}")
    print(f"   Œª learning rate: {base_crl['lambda_lr']:.1e}")
    
    return base_crl

def get_td3_crl_config(crl_profile="balanced"):
    """
    Get complete TD3 + CRL configuration.
    
    Args:
        crl_profile: CRL profile name for constraint settings
        
    Returns:
        Complete configuration dictionary for TD3 with CRL
    """
    crl_config = get_crl_config(crl_profile)
    complete_config = TD3_CONFIG.copy()
    complete_config.update(crl_config)
    
    return complete_config

def set_portfolio_mode(use_variable_portfolio):
    """Set the portfolio mode dynamically based on command line arguments."""
    global COINS_PER_EPISODE_RANGE, USE_VARIABLE_PORTFOLIO
    
    USE_VARIABLE_PORTFOLIO = use_variable_portfolio
    if use_variable_portfolio:
        COINS_PER_EPISODE_RANGE = (1, 3)  # Variable 1-3 coins from the 3-coin pool
        print(f"üîÑ Portfolio mode set to VARIABLE: {COINS_PER_EPISODE_RANGE[0]}-{COINS_PER_EPISODE_RANGE[1]} coins per episode")
    else:
        COINS_PER_EPISODE_RANGE = (3, 3)  # Fixed 3 coins
        print(f"üîÑ Portfolio mode set to FIXED: {COINS_PER_EPISODE_RANGE[0]}-{COINS_PER_EPISODE_RANGE[1]} coins per episode")

def get_random_coin_subset():
    """Get a random subset of coins for variable portfolio training."""
    import random
    min_coins, max_coins = COINS_PER_EPISODE_RANGE
    n_coins = random.randint(min_coins, min(max_coins, len(COIN_POOL)))
    return random.sample(COIN_POOL, n_coins)

def create_variable_obs_columns(selected_coins):
    """Create observation column names for selected coins."""
    obs_cols = []
    for coin in selected_coins:
        for scol in SCOLS:
            obs_cols.append(f"{coin}_{scol}")
    return obs_cols

def validate_config():
    """Validate that configuration is consistent."""
    if USE_VARIABLE_PORTFOLIO:
        assert MAX_COINS >= max(COINS_PER_EPISODE_RANGE), \
            "MAX_COINS must be >= maximum coins per episode"
        
        assert all(coin in COIN_POOL for coin in COINS), \
            "All legacy COINS must be in COIN_POOL"
        
        assert len(SINGLE_COIN_FEATURE_MIN) == FEATURES_PER_COIN, \
            "Single coin feature bounds must match FEATURES_PER_COIN"
        
        print(f"‚úÖ Variable portfolio configuration validated")
        print(f"üìä Max coins: {MAX_COINS}, Features per coin: {FEATURES_PER_COIN}")
        print(f"üé≤ Coins per episode: {COINS_PER_EPISODE_RANGE[0]}-{COINS_PER_EPISODE_RANGE[1]}")
        print(f"ü™ô Coin pool: {COIN_POOL}")
    else:
        # Legacy validation
        assert len(OBS_COLS) == len(OBS_COLS_MIN) == len(OBS_COLS_MAX), \
            "Observation columns and bounds must have same length for legacy mode"
        
        assert len(OBS_COLS) % N_COINS == 0, \
            "Number of observation features must be divisible by number of coins"
        
        print(f"‚úÖ Legacy configuration validated")
        print(f"üìä Data: {len(COINS)} coins, {len(OBS_COLS)} features ({FEATURES_PER_COIN_LEGACY} per coin)")
    
    print(f"üèãÔ∏è  Episode length: {EPISODE_LENGTH} steps")
    print(f"üí∞ Initial cash: ${INITIAL_CASH}")

if __name__ == "__main__":
    validate_config()





# #########################################################################################

# #########################################################################################

